# Install and load required packages
install.packages("randomForest")
library(sandwich)
library(randomForest)
library(dplyr)

# Define function using Random Forest
element.randomForest <- function(TNDdf_train){
  # Data splitting
  #TNDdf_train <- datagen(ssize = 1000, em = 0.5)
  s <- sample(1:length(TNDdf_train$Y), length(TNDdf_train$Y) / 2)
  TNDdf_train1 <- TNDdf_train[s,]
  TNDdf_train2 <- TNDdf_train[-s,]
  
  # Subset for controls in first split
  TNDdf_train_ctr1 <- subset(TNDdf_train1, Y == 0)
  
  # Train Random Forest on first control subset
  mod_g1_ctr <- randomForest(
    V ~ .,  
    data = subset(TNDdf_train_ctr1, select = -Y)
  )
  
  # Subset for controls in second split
  TNDdf_train_ctr2 <- subset(TNDdf_train2, Y == 0)
  
  # Train Random Forest on second control subset
  mod_g2_ctr <- randomForest(
    V ~ .,  
    data = subset(TNDdf_train_ctr2, select = -Y)
  )
  
  g1_cont <- TNDdf_train$V
  g1_cont[-s] <- predict(mod_g1_ctr, newdata = as.data.frame(cbind(select(TNDdf_train2, -c(V, Y)), V = rep(1, nrow(TNDdf_train2)), Y = TNDdf_train2$Y)))
  g1_cont[s] <- predict(mod_g2_ctr, newdata = as.data.frame(cbind(select(TNDdf_train1, -c(V, Y)), V = rep(1, nrow(TNDdf_train1)), Y = TNDdf_train1$Y)))
  
  # Train Random Forest models for mu
  Out_mu1 <- randomForest(
    Y ~ .,  
    data = TNDdf_train1
  )
  
  Out_mu2 <- randomForest(
    Y ~ .,  
    data = TNDdf_train2
  )
  
  mu1 <- TNDdf_train$Y
  mu0 <- TNDdf_train$Y
  mu1[-s] <- predict(Out_mu1, newdata = as.data.frame(cbind(V = 1, select(TNDdf_train2, -c(V, Y)))))
  mu1[s] <- predict(Out_mu2, newdata = as.data.frame(cbind(V = 1, select(TNDdf_train1, -c(V, Y)))))
  
  mu0[-s] <- predict(Out_mu1, newdata = as.data.frame(cbind(V = 0, select(TNDdf_train2, -c(V, Y)))))
  mu0[s] <- predict(Out_mu2, newdata = as.data.frame(cbind(V = 0, select(TNDdf_train1, -c(V, Y)))))
  
  # Train Random Forest models for m0
  Out_m1 <- randomForest(
    Y ~ .,  
    data = subset(TNDdf_train1, select = -V)
  )
  
  Out_m2 <- randomForest(
    Y ~ .,  
    data = subset(TNDdf_train2, select = -V)
  )
  
  m0 <- TNDdf_train$Y
  m0[-s] <- 1 - predict(Out_m1, newdata = select(TNDdf_train2, -c(V, Y)))
  m0[s] <- 1 - predict(Out_m2, newdata = select(TNDdf_train1, -c(V, Y)))
  
  return(list(mu1 = mu1, mu0 = mu0, m0 = m0, g1 = pmin(pmax(g1_cont, 0.01), 0.99), g0 = 1 - pmin(pmax(g1_cont, 0.01), 0.99),
              w1 = pmin(pmax(m0 / (1 - mu1), 0.01), 0.99), w0 = pmin(pmax(m0 / (1 - mu0), 0.01), 0.99)))
}


library(nnet)
library(dplyr)

# Define function using Neural Networks
element.nnet <- function(TNDdf_train){
  # Data splitting
  set.seed(123)  # Set seed for reproducibility
  s <- sample(1:length(TNDdf_train$Y), length(TNDdf_train$Y) / 2)
  TNDdf_train1 <- TNDdf_train[s,]
  TNDdf_train2 <- TNDdf_train[-s,]
  
  # Scale data for Neural Networks
  TNDdf_train1_scaled <- as.data.frame(scale(TNDdf_train1))
  TNDdf_train2_scaled <- as.data.frame(scale(TNDdf_train2))
  
  # Subset for controls in first split
  TNDdf_train_ctr1 <- subset(TNDdf_train1_scaled, Y == 0)
  
  # Train Neural Network on first control subset
  mod_g1_ctr <- nnet(
    V ~ .,  
    data = subset(TNDdf_train_ctr1, select = -Y),
    size = 5, maxit = 200
  )
  
  # Subset for controls in second split
  TNDdf_train_ctr2 <- subset(TNDdf_train2_scaled, Y == 0)
  
  # Train Neural Network on second control subset
  mod_g2_ctr <- nnet(
    V ~ .,  
    data = subset(TNDdf_train_ctr2, select = -Y),
    size = 5, maxit = 200
  )
  
  g1_cont <- TNDdf_train$V
  g1_cont[-s] <- predict(mod_g1_ctr, newdata = as.data.frame(cbind(select(TNDdf_train2, -c(V, Y)), V = rep(1, nrow(TNDdf_train2)), Y = TNDdf_train2$Y)), type = "raw")
  g1_cont[s] <- predict(mod_g2_ctr, newdata = as.data.frame(cbind(select(TNDdf_train1, -c(V, Y)), V = rep(1, nrow(TNDdf_train1)), Y = TNDdf_train1$Y)), type = "raw")
  
  # Train Neural Network models for mu
  Out_mu1 <- nnet(
    Y ~ .,  
    data = TNDdf_train1_scaled,
    size = 5, maxit = 200
  )
  
  Out_mu2 <- nnet(
    Y ~ .,  
    data = TNDdf_train2_scaled,
    size = 5, maxit = 200
  )
  
  mu1 <- TNDdf_train$Y
  mu0 <- TNDdf_train$Y
  mu1[-s] <- predict(Out_mu1, newdata = as.data.frame(cbind(V = 1, select(TNDdf_train2, -c(V, Y)))), type = "raw")
  mu1[s] <- predict(Out_mu2, newdata = as.data.frame(cbind(V = 1, select(TNDdf_train1, -c(V, Y)))), type = "raw")
  
  mu0[-s] <- predict(Out_mu1, newdata = as.data.frame(cbind(V = 0, select(TNDdf_train2, -c(V, Y)))), type = "raw")
  mu0[s] <- predict(Out_mu2, newdata = as.data.frame(cbind(V = 0, select(TNDdf_train1, -c(V, Y)))), type = "raw")
  
  # Train Neural Network models for m0
  Out_m1 <- nnet(
    Y ~ .,  
    data = subset(TNDdf_train1_scaled, select = -V),
    size = 5, maxit = 200
  )
  
  Out_m2 <- nnet(
    Y ~ .,  
    data = subset(TNDdf_train2_scaled, select = -V),
    size = 5, maxit = 200
  )
  
  m0 <- TNDdf_train$Y
  m0[-s] <- 1 - predict(Out_m1, newdata = select(TNDdf_train2, -c(V, Y)), type = "raw")
  m0[s] <- 1 - predict(Out_m2, newdata = select(TNDdf_train1, -c(V, Y)), type = "raw")
  
  return(list(mu1 = mu1, mu0 = mu0, m0 = m0, g1 = pmin(pmax(g1_cont, 0.01), 0.99), g0 = 1 - pmin(pmax(g1_cont, 0.01), 0.99),
              w1 = pmin(pmax(m0 / (1 - mu1), 0.01), 0.99), w0 = pmin(pmax(m0 / (1 - mu0), 0.01), 0.99)))
}



######## Methods for VE
### IPW estimator
mod_IPW <- function(TNDdat, res){
  TNDdat$ipw <- ifelse(TNDdat$V == 1, 1/res$g1, 1/res$g0)
  modY.ipw <- glm(Y ~ V, family=binomial(link = "log"), weights = ipw, data=TNDdat)
  est.ipw <- exp(modY.ipw$coefficients[2])
  se.ipw <- sqrt(vcovHC(modY.ipw)[2,2])
  
  CI_l <- est.ipw *exp(- 1.96 * se.ipw )
  CI_u <- est.ipw *exp( 1.96 * se.ipw )
  return(list(est = est.ipw, se = se.ipw, CI =  c(CI_l, CI_u)))
}
### EIF based estimator with OUTCOME ratio debiasing weights
mod_EIF1OUT <- function(TNDdat, res){
  # proposed eif estimator 1 with Out ratio weights
  A.1 <- res$w1*res$mu1*((1 - TNDdat$Y)*(TNDdat$V - res$g1))/(res$g1* res$m0)
  A.0 <- res$w0*res$mu0*((1 - TNDdat$Y)*((1-TNDdat$V) - res$g0))/(res$g0* res$m0)
  psi.1 <- mean(TNDdat$Y*TNDdat$V/res$g1 - A.1)
  psi.0 <- mean(TNDdat$Y*(1-TNDdat$V)/res$g0 - A.0)
  mod_eif <- psi.1/psi.0
  eifln <-  ((TNDdat$Y*TNDdat$V/res$g1 - A.1)/psi.1) - ((TNDdat$Y*(1-TNDdat$V)/res$g0 - A.0)/ psi.0)
  varln <-  var(eifln)/nrow(TNDdat)
  
  CI_l1 <- exp(log(mod_eif) - 1.96 * sqrt(varln) )
  CI_u1 <- exp(log(mod_eif) + 1.96 * sqrt(varln) )
  
  eifpsi <- (TNDdat$Y*TNDdat$V/res$g1 - A.1)/psi.0 - (psi.1/psi.0)*(TNDdat$Y*(1-TNDdat$V)/res$g0 - A.0)/psi.0
  var <- var(eifpsi)/nrow(TNDdat)
  CI_l2 <- mod_eif - 1.96 * sqrt(var) 
  CI_u2 <- mod_eif + 1.96 * sqrt(var) 
  return(list( est = mod_eif, varln = varln, var = var, CI1 =  c(CI_l1, CI_u1), CI2 =  c(CI_l2, CI_u2)))
}
### EIF based estimator with PS ratio debiasing weights
mod_EIF1PS <- function(TNDdat, res){
  # proposed eif estimator 1 with PS ratio weights
  A.1 <- res$w1.ps*res$mu1*((1 - TNDdat$Y)*(TNDdat$V - res$g1))/(res$g1* res$m0)
  A.0 <- res$w0.ps*res$mu0*((1 - TNDdat$Y)*((1-TNDdat$V) - res$g0))/(res$g0* res$m0)
  psi.1 <- mean(TNDdat$Y*TNDdat$V/res$g1 - A.1)
  psi.0 <- mean(TNDdat$Y*(1-TNDdat$V)/res$g0 - A.0)
  mod_eif <- psi.1/psi.0
  eifln <-  ((TNDdat$Y*TNDdat$V/res$g1 - A.1)/psi.1) - ((TNDdat$Y*(1-TNDdat$V)/res$g0 - A.0)/ psi.0)
  varln <-  var(eifln)/nrow(TNDdat)
  
  CI_l1 <- exp(log(mod_eif) - 1.96 * sqrt(varln) )
  CI_u1 <- exp(log(mod_eif) + 1.96 * sqrt(varln) )
  
  eifpsi <- (TNDdat$Y*TNDdat$V/res$g1 - A.1)/psi.0 - (psi.1/psi.0)*(TNDdat$Y*(1-TNDdat$V)/res$g0 - A.0)/psi.0
  var <- var(eifpsi)/nrow(TNDdat)
  CI_l2 <- mod_eif - 1.96 * sqrt(var) 
  CI_u2 <- mod_eif + 1.96 * sqrt(var) 
  return(list( est = mod_eif, varln = varln, var = var, CI1 =  c(CI_l1, CI_u1), CI2 =  c(CI_l2, CI_u2)))
}

#### proposed TNDDR, SAME as eif1OUT
mod_EIF2 <- function(TNDdat, res){
  A.1 <- ((1 - TNDdat$Y)*(TNDdat$V - res$g1))/(res$g1* (1 -res$mu1))
  A.0 <- ((1 - TNDdat$Y)*((1-TNDdat$V) - res$g0))/(res$g0* (1 - res$mu0))
  psi.1 <- mean(TNDdat$Y*TNDdat$V/res$g1 - res$mu1*A.1)
  psi.0 <- mean(TNDdat$Y*(1-TNDdat$V)/res$g0 - res$mu0*A.0)
  mod_eif2 <- psi.1/psi.0
  eifln <-  ((TNDdat$Y*TNDdat$V/res$g1 - res$mu1*A.1)/psi.1) - ((TNDdat$Y*(1-TNDdat$V)/res$g0 - res$mu0*A.0)/ psi.0)
  varln <-  var(eifln)/nrow(TNDdat)
  
  CI_l1 <- exp(log(mod_eif2) - 1.96 * sqrt(varln) )
  CI_u1 <- exp(log(mod_eif2) + 1.96 * sqrt(varln) )
  
  eifpsi <- (TNDdat$Y*TNDdat$V/res$g1 - res$mu1*A.1)/psi.0 - (psi.1/psi.0)*(TNDdat$Y*(1-TNDdat$V)/res$g0 - res$mu0*A.0)/psi.0
  var <- var(eifpsi)/nrow(TNDdat)
  CI_l2 <- mod_eif2 - 1.96 * sqrt(var) 
  CI_u2 <- mod_eif2 + 1.96 * sqrt(var) 
  return(list( est = mod_eif2, varln = varln, var = var, CI1 =  c(CI_l1, CI_u1), CI2 =  c(CI_l2, CI_u2)))
}

mod_OutReg<- function(TNDdat, res, method = "RandomForest"){
  mod_OR1 <- mean(res$mu1 * res$w1)/mean(res$mu0 * res$w0)
  nbs <- 50
  bsest<-rep(NA,nbs)
  for(i in 1:nbs){
    resamps<-sample(1:nrow(TNDdat),size=nrow(TNDdat),replace=T)
    datk<-TNDdat[resamps,]
    # Fit model based on the chosen method
    if (method == "HAL") {
      # HAL using glmnet (Highly Adaptive Lasso)
      res <- element.HAL(datk)
    } else if (method == "Earth") {
      # Earth model (MARS - Multivariate Adaptive Regression Splines)
      res <- element.earth(datk)
    } else if (method == "RandomForest") {
      # Random Forest
      res <- element.randomForest(datk)
    } else if (method == "NN") {
      # Neural Network
      res <- element.nnet(datk)
    } else {
      stop("Unsupported method. Please choose from 'HAL', 'Earth', 'RandomForest', or 'NN'.")
    }
    bsest[i] <- mean(res$mu1 * res$w1)/mean(res$mu0 * res$w0)
  }
  bs_var <- var(bsest)
  CI = quantile(bsest,c(0.025,0.975))
  return(list(est = mod_OR1, CI.OR = CI))
}



